from typing import List, Tuple

import requests
import logging
import random
import json
import sys
import re
import os

import pandas as pd
import mlflow

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

PATH_DATA   = os.getenv('PATH_DATA' , '/app/result.pkl'               )
RASA_URL    = os.getenv('RASA_URL'  , 'http://localhost:5005/webhooks/rest/webhook' )
STAGE       = os.getenv('STAGE'     , 'dev'                                         )


def _read_data() -> Tuple[List, List]:
    '''Read the data of questions for querying against the chatbot.

    Returns:
        Tuple[List, List]: List of questions along with correct answers (URLs).
    '''
    df = pd.read_pickle(PATH_DATA)
    
    questions   = df['Question'].values.tolist()
    answers     = df['URL'].values.tolist()

    return (questions, answers)


def _get_results(questions: List) -> List:
    '''Query the list of questions against the chatbot in development environment.

    Args:
        questions (List): List of questions.

    Returns:
        List: Returns the list of answers generated by the chatbot.
    '''

    SENDER  = random.randint(0, 1000000)
    DATA    = {
        'message'   : '', 
        'sender'    : str(SENDER)
    }

    results = []

    logger.info(f'Querying {len(questions)} questions against chatbot...')
    for i, q in enumerate(questions):

        DATA['message'] = q
        try:
            response = requests.post(RASA_URL, json = DATA)
            if response.status_code != 200:
                logger.error(f'Error: Service at {RASA_URL} is unavailable, exit.')
                exit()

        except Exception as e:
            logger.error(f'Error: Exception at posting question - "{q}", exit. {type(e).__name__}: "{e}".')
            exit()

        
        try:
            r = json.loads(response.text)
            success = False
            for link in r:
                if 'custom' in link:
                    success = True
                    r = link['custom']
                    break
        except Exception as e:
            logger.error(f'Error: Failed on parsing response on question - "{q}", exit. {type(e).__name__}: "{e}".')
            exit()

        
        result = []
        if success:
            try:
                r = r['data']
                if len(r) == 0:
                    raise Exception
            except Exception as e:
                logger.error(f'Error: Failed on parsing response on question - "{m}", exit. . {type(e).__name__}: "{e}".')

            for e in r:
                title   = re.findall("<em>(.*?)</em>"           , e['title'])[0]
                link    = re.findall("href=[\"\'](.*?)[\"\']"   , e['title'])[0]
                result.append((title, link))
            

            DATA['message'] = '/intent_affirm'
            try:
                response = requests.post(RASA_URL, json = DATA)
                if response.status_code != 200:
                    logger.error(f'Error: Service at {RASA_URL} is unavailable, exit.')
                    exit()
            except Exception as e:
                logger.error(f'Error: Exception at posting affirmative message on question - "{q}", exit. {type(e).__name__}: "{e}".')
                exit()
            
            try:
                r = json.loads(response.text)
                r = r[0]
                r = r['text']
                if 'Anything else I can help with?' != r:
                    raise Exception
            except Exception as e:
                logger.error(f'Error: Failed on parsing response of affirmative message on question - "{q}", exit. {type(e).__name__}: "{e}".')
                exit()
        else:
            logger.info(f'No results for question - "{q}"')
            result.append(('', ''))
            
        results.append(result)
        if (i+1)%5 == 0:
            logger.info(f'Finished {i+1} questions...')
    
    logger.info(f'Finished querying all questions for scoring')
    return results

def _get_scores(
    answers: List, 
    results: List
    ) -> List:
    '''Get scores for the messages - in top 1, 3, 5 and 10 results from the chatbot.

    Args:
        answers (List): Correct URLS to questions.
        results (List): Correspondong results from chatbot.

    Returns:
        List: List of scores in boolean triples (for top 1, 3, and 5 hits correspondingly).
    '''
    scores = []
    for i, r in enumerate(results):
        answer = answers[i]
        topn = [False, False, False, False]
        for i1, r1 in enumerate(r):
            if r1[1].split('?')[0] in answer:
                if i1 == 0:
                    topn[0] = True
                if i1 < 3:
                    topn[1] = True
                if i1 < 5:
                    topn[2] = True
                topn[3] = True
        scores.append(topn)
    
    return scores

def print_metrics(scores: List) -> None:
    '''Prints out the scores.

    Args:
        scores (List): Score for the questions.
    '''
    top1  = 0
    top3  = 0
    top5  = 0
    top10 = 0
    for topn in scores:
        if topn[0]: top1    += 1
        if topn[1]: top3    += 1
        if topn[2]: top5    += 1
        if topn[3]: top10   += 1

    logger.info(f'Out of {len(scores)} results, following correct:'  )
    logger.info(f'Top 1 : {top1 :<3d} ({top1 /len(scores) * 100:<.2f}%)')
    logger.info(f'Top 3 : {top3 :<3d} ({top3 /len(scores) * 100:<.2f}%)')
    logger.info(f'Top 5 : {top5 :<3d} ({top5 /len(scores) * 100:<.2f}%)')
    logger.info(f'Top 10: {top10:<3d} ({top10/len(scores) * 100:<.2f}%)')

    # Set MLflow experiment name (MLFLOW_EXPERIMENT_NAME is set in the workflow or docker-compose)
    mlflow.set_experiment(os.getenv('MLFLOW_EXPERIMENT_NAME'))
    # Start an MLflow run
    with mlflow.start_run() as run:
        # Log evaluation results to MLflow
        mlflow.log_metric("top1", top1/len(scores) * 100)
        mlflow.log_metric("top3", top3/len(scores) * 100)
        mlflow.log_metric("top5", top5/len(scores) * 100)
        mlflow.log_metric("top10", top10/len(scores) * 100)
        mlflow.log_param("test_set", os.getenv('TEST_SET')) # Whether it is the UCIPM or AskExtension test dataset
        mlflow.log_param("description", os.getenv('DESCRIPTION')) # Short description of is being evaluated

def main():

    logger.info(f'RASA ENDPOINT : {RASA_URL}')    
    logger.info(f'DATA PATH     : {PATH_DATA}')    
    logger.info(f'ENVIRONMENT   : {STAGE}')    
    
    questions, answers  = _read_data    ()
    results             = _get_results  (questions)
    scores              = _get_scores   (answers, results)

    print_metrics(scores)


if __name__ == "__main__":
    main()
